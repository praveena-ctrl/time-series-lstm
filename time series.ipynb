{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfe02ec6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'null' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m {\n\u001b[32m      2\u001b[39m  \u001b[33m\"\u001b[39m\u001b[33mcells\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m      3\u001b[39m   {\n\u001b[32m      4\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mexecution_count\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mnull\u001b[49m,\n\u001b[32m      6\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m941de11d-01f5-4057-be26-82441549e2d1\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      7\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m      8\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m      9\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m     10\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     11\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mAdvanced Time Series Forecasting with Deep Learning: LSTM Optimization and Interpretability\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     12\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     13\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mThis single-file implementation covers:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     14\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- Programmatic dataset generation (2000 daily observations)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     15\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- Preprocessing and sequence dataset creation\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     16\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- PyTorch LSTM model with AdamW optimizer and cosine annealing LR schedule\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     17\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- Dropout scheduling (increasing dropout during training) and weight decay\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     18\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- Baseline models: SARIMA (statsmodels) and a simple Feedforward NN (PyTorch)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     19\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- Evaluation: RMSE, MAE, MAPE on a hold-out test set\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     20\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- Interpretability: permutation importance over temporal features and a simple SHAP-like approximation\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     21\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- Saves dataset to ./lstm_timeseries_dataset.csv\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     22\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     23\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mRun with: python Advanced_LSTM_Forecasting_Project.py\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     24\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     25\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mDependencies:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     26\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- numpy, pandas, scipy, scikit-learn, torch, statsmodels, matplotlib\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     27\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- Optional: shap (only for advanced users)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     28\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     29\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     30\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     31\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mimport os\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     32\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mimport math\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     33\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mimport random\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     34\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mimport numpy as np\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     35\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mimport pandas as pd\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     36\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfrom pathlib import Path\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     37\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfrom sklearn.preprocessing import StandardScaler\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     38\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfrom sklearn.metrics import mean_squared_error, mean_absolute_error\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     39\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mimport matplotlib.pyplot as plt\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     40\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     41\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m# PyTorch imports\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     42\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mimport torch\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     43\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mimport torch.nn as nn\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     44\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfrom torch.utils.data import Dataset, DataLoader\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     45\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     46\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m# Statsmodels for SARIMA baseline\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     47\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mimport statsmodels.api as sm\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     48\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     49\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m# -----------------------------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     50\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m# 1) DATA GENERATION + SAVE\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     51\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m# -----------------------------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     52\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     53\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdef generate_dataset(n=2000, seed=42, save_path=\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m./lstm_timeseries_dataset.csv\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     54\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    np.random.seed(seed)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     55\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    date_index = pd.date_range(start=\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m2000-01-01\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m, periods=n, freq=\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mD\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     56\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     57\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    # Components\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     58\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    trend = 0.0005 * np.arange(n)               # slow linear trend\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     59\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    seasonal_yearly = 2.0 * np.sin(2 * np.pi * np.arange(n) / 365.25)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     60\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    seasonal_weekly = 0.5 * np.sin(2 * np.pi * np.arange(n) / 7.0)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     61\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    ar_component = np.zeros(n)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     62\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    phi = [0.6, -0.2]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     63\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    for t in range(2, n):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     64\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        ar_component[t] = phi[0] * ar_component[t-1] + phi[1] * ar_component[t-2]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     65\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     66\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    exog1 = 0.8 * np.sin(2 * np.pi * np.arange(n) / 90.0) + 0.2 * np.random.normal(size=n)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     67\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    exog2 = np.cos(2 * np.pi * np.arange(n) / 30.0) + 0.1 * np.random.normal(size=n)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     68\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    exog3 = np.random.normal(scale=0.5, size=n)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     69\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     70\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    noise = np.random.normal(scale=0.8, size=n)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     71\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    target = 3.0 + trend + seasonal_yearly + seasonal_weekly + 1.5 * ar_component + 0.7 * exog1 - 0.4 * exog2 + 0.3 * exog3 + noise\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     72\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     73\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    max_lag = 14\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     74\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    lags = \u001b[39m\u001b[33m{\u001b[39m\u001b[33mf\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mlag_\u001b[39m\u001b[38;5;132;01m{lag}\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m: pd.Series(target).shift(lag).fillna(method=\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mbfill\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m).values for lag in range(1, max_lag+1)}\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     75\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     76\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    df = pd.DataFrame(\u001b[39m\u001b[33m{\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     77\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mtimestamp\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m: date_index,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     78\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mtarget\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m: target,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     79\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mexog1\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m: exog1,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     80\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mexog2\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m: exog2,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     81\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mexog3\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m: exog3,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     82\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mtrend\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m: trend,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     83\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mseasonal_yearly\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m: seasonal_yearly,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     84\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mseasonal_weekly\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m: seasonal_weekly\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     85\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    })\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     86\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    for k, v in lags.items():\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     87\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        df[k] = v\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     88\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     89\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    df.to_csv(save_path, index=False)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     90\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    print(f\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mDataset saved to: \u001b[39m\u001b[38;5;132;01m{save_path}\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     91\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    return df\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     92\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     93\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m# -----------------------------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     94\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m# 2) PYTORCH SEQUENCE DATASET\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     95\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m# -----------------------------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     96\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     97\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mclass SequenceDataset(Dataset):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    def __init__(self, df, feature_cols, target_col=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mtarget\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, seq_len=30):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     99\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        self.feature_cols = feature_cols\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    100\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        self.target_col = target_col\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    101\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        self.seq_len = seq_len\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    102\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        self.X = df[feature_cols].values.astype(np.float32)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    103\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        self.y = df[target_col].values.astype(np.float32)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    104\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    105\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    def __len__(self):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    106\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        return len(self.y) - self.seq_len\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    107\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    108\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    def __getitem__(self, idx):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    109\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        x = self.X[idx: idx + self.seq_len]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    110\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        y = self.y[idx + self.seq_len]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    111\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        return x, y\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    112\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    113\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m# -----------------------------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    114\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m# 3) LSTM MODEL (PyTorch)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    115\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m# -----------------------------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    116\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    117\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mclass LSTMForecast(nn.Module):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    118\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    def __init__(self, n_features, hidden_size=64, num_layers=2, dropout=0.2, bidirectional=False):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    119\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        super().__init__()\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    120\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        self.lstm = nn.LSTM(input_size=n_features, hidden_size=hidden_size,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    121\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m                            num_layers=num_layers, batch_first=True, dropout=dropout, bidirectional=bidirectional)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    122\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        direction = 2 if bidirectional else 1\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    123\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        self.fc = nn.Sequential(\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    124\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m            nn.Linear(hidden_size * direction, 64),\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    125\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m            nn.ReLU(),\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    126\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m            nn.Dropout(dropout),\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    127\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m            nn.Linear(64, 1)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    128\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        )\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    129\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    130\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    def forward(self, x):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    131\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        # x: (batch, seq_len, features)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    132\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        out, _ = self.lstm(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    133\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        out = out[:, -1, :]  # last time step\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    134\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        out = self.fc(out)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    135\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        return out.squeeze(1)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    136\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    137\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m# -----------------------------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    138\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m# Utility: train / evaluate\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    139\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m# -----------------------------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    140\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    141\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdef train_model(model, train_loader, val_loader, epochs=50, lr=1e-3, weight_decay=1e-4, device=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, dropout_schedule=None):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    142\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    model.to(device)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    143\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    144\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    # Cosine annealing scheduler\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    145\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    146\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    loss_fn = nn.MSELoss()\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    147\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    148\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    best_val = float(\u001b[39m\u001b[33m'\u001b[39m\u001b[33minf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    149\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    best_state = None\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    150\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    151\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    for ep in range(1, epochs+1):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    152\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        model.train()\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    153\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        train_losses = []\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    154\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        # optional dropout schedule: a function mapping epoch->dropout value\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    155\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        if dropout_schedule is not None and hasattr(model, \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlstm\u001b[39m\u001b[33m'\u001b[39m\u001b[33m):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    156\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m            new_dropout = float(dropout_schedule(ep))\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    157\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m            # update dropout in LSTM and fc dropout layers (if present)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    158\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m            try:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    159\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m                model.lstm.dropout = new_dropout\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    160\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m            except Exception:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    161\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m                pass\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    162\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m            # update fc dropout layers\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    163\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m            for m in model.modules():\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    164\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m                if isinstance(m, nn.Dropout):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    165\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m                    m.p = new_dropout\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    166\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    167\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        for xb, yb in train_loader:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    168\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m            xb = xb.to(device)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    169\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m            yb = yb.to(device)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    170\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m            optimizer.zero_grad()\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    171\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m            preds = model(xb)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    172\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m            loss = loss_fn(preds, yb)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    173\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m            loss.backward()\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    174\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m            optimizer.step()\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    175\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m            train_losses.append(loss.item())\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    176\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    177\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        scheduler.step()\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    178\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    179\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        # validation\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    180\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        model.eval()\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    181\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        val_losses = []\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    182\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        with torch.no_grad():\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    183\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m            for xb, yb in val_loader:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    184\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m                xb = xb.to(device)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    185\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m                yb = yb.to(device)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    186\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m                preds = model(xb)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    187\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m                val_losses.append(mean_squared_error(yb.cpu().numpy(), preds.cpu().numpy()))\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    188\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        val_rmse = math.sqrt(np.mean(val_losses))\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    189\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    190\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        if val_rmse < best_val:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    191\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m            best_val = val_rmse\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    192\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m            best_state = model.state_dict()\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    193\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    194\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        if ep \u001b[39m\u001b[33m%\u001b[39m\u001b[33m 10 == 0 or ep == 1:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    195\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m            print(f\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{ep}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{epochs}\u001b[39;00m\u001b[33m - train_loss: \u001b[39m\u001b[33m{\u001b[39m\u001b[33mnp.mean(train_losses):.4f} - val_rmse: \u001b[39m\u001b[38;5;132;01m{val_rmse:.4f}\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    196\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    197\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    if best_state is not None:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    198\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        model.load_state_dict(best_state)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    199\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    return model\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    200\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    201\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m# -----------------------------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    202\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m# Baselines: SARIMA and FFNN\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    203\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m# -----------------------------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    204\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    205\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdef sarima_forecast(train_series, test_steps, order=(2,0,1), seasonal_order=(1,1,1,365)):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    206\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    # Note: seasonal_order\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms seasonal_periods is large (365) because daily data has yearly seasonality\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    207\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    model = sm.tsa.statespace.SARIMAX(train_series, order=order, seasonal_order=seasonal_order, enforce_stationarity=False, enforce_invertibility=False)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    208\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    res = model.fit(disp=False)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    209\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    fc = res.get_forecast(steps=test_steps).predicted_mean\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    210\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    return fc\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    211\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    212\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mclass FFNN(nn.Module):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    213\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    def __init__(self, input_dim, hidden=128, dropout=0.2):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    214\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        super().__init__()\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    215\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        self.net = nn.Sequential(\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    216\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m            nn.Linear(input_dim, hidden),\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    217\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m            nn.ReLU(),\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    218\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m            nn.Dropout(dropout),\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    219\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m            nn.Linear(hidden, hidden//2),\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    220\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m            nn.ReLU(),\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    221\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m            nn.Linear(hidden//2, 1)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    222\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        )\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    223\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    def forward(self, x):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        return self.net(x).squeeze(1)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m# -----------------------------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m# Permutation Importance for sequences\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    228\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m# -----------------------------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    229\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    230\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdef permutation_importance_sequence(model, X_test, y_test, feature_idx_to_permute, seq_len=30, device=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m\u001b[33m):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    231\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    # X_test: numpy array shape (n_samples, seq_len, n_features)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    232\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    baseline_preds = model(torch.tensor(X_test, dtype=torch.float32).to(device)).detach().cpu().numpy()\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    233\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    baseline_rmse = math.sqrt(mean_squared_error(y_test, baseline_preds))\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    234\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    235\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    X_perm = X_test.copy()\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    236\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    # permute along the sample axis while preserving time order inside each sample for that feature index\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    237\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    for i in range(X_perm.shape[0]):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    238\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        # shuffle the feature values across samples\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    239\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        X_perm[:, :, feature_idx_to_permute] = np.random.permutation(X_perm[:, :, feature_idx_to_permute])\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    240\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    241\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    perm_preds = model(torch.tensor(X_perm, dtype=torch.float32).to(device)).detach().cpu().numpy()\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    242\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    perm_rmse = math.sqrt(mean_squared_error(y_test, perm_preds))\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    243\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    importance = perm_rmse - baseline_rmse\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    244\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    return importance, baseline_rmse, perm_rmse\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    245\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    246\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m# -----------------------------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    247\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m# FULL PIPELINE: load/generate data, preprocess, train, evaluate\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    248\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m# -----------------------------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    249\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    250\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdef run_full_pipeline(csv_path=\u001b[39m\u001b[33m'\u001b[39m\u001b[33m./lstm_timeseries_dataset.csv\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, seq_len=30, test_size=0.2, device=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m\u001b[33m):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    251\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    # ensure dataset exists\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    252\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    if not os.path.exists(csv_path):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    253\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        print(\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mDataset not found locally â€” generating now...\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    254\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        df = generate_dataset(save_path=csv_path)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    255\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    else:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    256\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        df = pd.read_csv(csv_path, parse_dates=[\u001b[39m\u001b[33m'\u001b[39m\u001b[33mtimestamp\u001b[39m\u001b[33m'\u001b[39m\u001b[33m])\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    257\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    258\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    # features to use for sequence model (exclude timestamp and target)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    259\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    feature_cols = [c for c in df.columns if c not in [\u001b[39m\u001b[33m'\u001b[39m\u001b[33mtimestamp\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33mtarget\u001b[39m\u001b[33m'\u001b[39m\u001b[33m]]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    260\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    261\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    # Split train/val/test by time (no shuffling)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    262\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    n = len(df)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    263\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    test_n = int(n * test_size)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    264\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    train_val_n = n - test_n\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    265\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    val_n = int(train_val_n * 0.1)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    266\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    train_n = train_val_n - val_n\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    267\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    268\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    train_df = df.iloc[:train_n].copy()\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    269\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    val_df = df.iloc[train_n:train_n+val_n].copy()\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    270\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    test_df = df.iloc[train_n+val_n:].copy()\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    271\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    272\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    # scaler fitted on train features only\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    273\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    scaler = StandardScaler()\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    274\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    scaler.fit(train_df[feature_cols].values)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    275\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    276\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    for d in [train_df, val_df, test_df]:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    277\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        d[feature_cols] = scaler.transform(d[feature_cols].values)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    278\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    279\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    train_ds = SequenceDataset(train_df, feature_cols, seq_len=seq_len)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    280\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    val_ds = SequenceDataset(val_df, feature_cols, seq_len=seq_len)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    281\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    test_ds = SequenceDataset(test_df, feature_cols, seq_len=seq_len)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    282\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    283\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    batch_size = 64\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    284\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    285\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    286\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    287\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    288\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    n_features = len(feature_cols)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    289\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    model = LSTMForecast(n_features=n_features, hidden_size=128, num_layers=2, dropout=0.2, bidirectional=False)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    290\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    291\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    # dropout schedule: increase dropout slowly to regularize later epochs\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    292\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    def dropout_schedule(epoch):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    293\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        return min(0.5, 0.1 + (epoch / 100) * 0.4)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    294\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    295\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    model = train_model(model, train_loader, val_loader, epochs=60, lr=1e-3, weight_decay=1e-4, device=device, dropout_schedule=dropout_schedule)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    296\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    297\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    # Evaluate on test set\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    298\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    model.to(device)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    299\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    model.eval()\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    300\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    X_test = []\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    301\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    y_test = []\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    302\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    with torch.no_grad():\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    303\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        for xb, yb in test_loader:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    304\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m            X_test.append(xb.numpy())\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    305\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m            y_test.append(yb.numpy())\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    306\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    X_test = np.concatenate(X_test, axis=0)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    307\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    y_test = np.concatenate(y_test, axis=0)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    308\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    preds = model(torch.tensor(X_test, dtype=torch.float32).to(device)).detach().cpu().numpy()\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    309\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    310\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    rmse = math.sqrt(mean_squared_error(y_test, preds))\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    311\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    mae = mean_absolute_error(y_test, preds)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    312\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    mape = np.mean(np.abs((y_test - preds) / (y_test + 1e-8))) * 100\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    313\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    314\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    print(f\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mLSTM Test RMSE: \u001b[39m\u001b[38;5;132;01m{rmse:.4f}\u001b[39;00m\u001b[33m, MAE: \u001b[39m\u001b[38;5;132;01m{mae:.4f}\u001b[39;00m\u001b[33m, MAPE: \u001b[39m\u001b[38;5;132;01m{mape:.2f}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    315\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    316\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    # Baseline 1: SARIMA trained on the unscaled target series (last portion as test)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    317\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    train_series = df[\u001b[39m\u001b[33m'\u001b[39m\u001b[33mtarget\u001b[39m\u001b[33m'\u001b[39m\u001b[33m].iloc[:train_n+val_n]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    318\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    test_steps = len(test_df) - seq_len\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    319\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    try:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    320\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        sarima_pred = sarima_forecast(train_series.values, test_steps=test_steps)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    321\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        # align length\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    322\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        sarima_y = df[\u001b[39m\u001b[33m'\u001b[39m\u001b[33mtarget\u001b[39m\u001b[33m'\u001b[39m\u001b[33m].iloc[train_n+val_n+seq_len:train_n+val_n+seq_len+test_steps].values\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    323\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        sarima_rmse = math.sqrt(mean_squared_error(sarima_y, sarima_pred))\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    324\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        print(f\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mSARIMA Test RMSE (approx): \u001b[39m\u001b[38;5;132;01m{sarima_rmse:.4f}\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    325\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    except Exception as e:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    326\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        print(\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mSARIMA failed (likely because statsmodels couldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt converge).\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m, e)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    327\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    328\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    # Baseline 2: Feedforward NN on lag features (flatten last seq)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    329\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    # Prepare flattened lag features: use last seq_len timesteps flattened\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    330\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    X_flat = []\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    331\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    y_flat = []\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    332\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    for i in range(len(df) - seq_len):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    333\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        X_flat.append(df[feature_cols].values[i:i+seq_len].flatten())\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    334\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        y_flat.append(df[\u001b[39m\u001b[33m'\u001b[39m\u001b[33mtarget\u001b[39m\u001b[33m'\u001b[39m\u001b[33m].values[i+seq_len])\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    335\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    X_flat = np.array(X_flat)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    336\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    y_flat = np.array(y_flat)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    337\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    # train/val/test splits aligned\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    338\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    X_train, X_val, X_test_flat = X_flat[:train_n-seq_len], X_flat[train_n-seq_len:train_n+val_n-seq_len], X_flat[train_n+val_n-seq_len:]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    339\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    y_train, y_val, y_test_flat = y_flat[:train_n-seq_len], y_flat[train_n-seq_len:train_n+val_n-seq_len], y_flat[train_n+val_n-seq_len:]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    340\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    341\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    ffnn = FFNN(input_dim=X_train.shape[1], hidden=256, dropout=0.3)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    342\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    optimizer = torch.optim.AdamW(ffnn.parameters(), lr=1e-3, weight_decay=1e-4)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    343\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    loss_fn = nn.MSELoss()\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    344\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    ffnn.to(device)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    345\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    # small training loop\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    346\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    for epoch in range(25):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    347\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        ffnn.train()\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    348\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        perm = np.random.permutation(len(X_train))\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    349\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        batch = 128\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    350\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        losses = []\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    351\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        for i in range(0, len(perm), batch):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    352\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m            idx = perm[i:i+batch]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    353\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m            xb = torch.tensor(X_train[idx], dtype=torch.float32).to(device)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    354\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m            yb = torch.tensor(y_train[idx], dtype=torch.float32).to(device)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    355\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m            optimizer.zero_grad()\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    356\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m            preds = ffnn(xb)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    357\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m            loss = loss_fn(preds, yb)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    358\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m            loss.backward()\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    359\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m            optimizer.step()\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    360\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m            losses.append(loss.item())\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    361\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        if epoch \u001b[39m\u001b[33m%\u001b[39m\u001b[33m 5 == 0:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    362\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m            print(f\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mFFNN epoch \u001b[39m\u001b[38;5;132;01m{epoch}\u001b[39;00m\u001b[33m loss: \u001b[39m\u001b[33m{\u001b[39m\u001b[33mnp.mean(losses):.4f}\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    363\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    364\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    ffnn.eval()\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    365\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    with torch.no_grad():\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    366\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        pred_ffnn = ffnn(torch.tensor(X_test_flat, dtype=torch.float32).to(device)).cpu().numpy()\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    367\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    ffnn_rmse = math.sqrt(mean_squared_error(y_test_flat, pred_ffnn))\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    368\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    print(f\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mFFNN (lag-flattened) RMSE: \u001b[39m\u001b[38;5;132;01m{ffnn_rmse:.4f}\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    369\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    370\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    # Permutation importance on LSTM (feature-level)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    371\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    print(\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mComputing permutation importances (this may take a little while)...\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    372\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    importances = \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    373\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    for i, feat in enumerate(feature_cols):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    374\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        imp, base, perm = permutation_importance_sequence(model, X_test, y_test, feature_idx_to_permute=i, seq_len=seq_len, device=device)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    375\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        importances[feat] = imp\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    376\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    sorted_imps = sorted(importances.items(), key=lambda x: -abs(x[1]))\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    377\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    print(\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mPermutation importances (feature -> delta RMSE):\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    378\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    for feat, imp in sorted_imps:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    379\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        print(f\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m  \u001b[39m\u001b[38;5;132;01m{feat}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{imp:.4f}\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    380\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    381\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    results = \u001b[39m\u001b[33m{\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    382\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlstm_rmse\u001b[39m\u001b[33m'\u001b[39m\u001b[33m: rmse,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    383\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        \u001b[39m\u001b[33m'\u001b[39m\u001b[33mffnn_rmse\u001b[39m\u001b[33m'\u001b[39m\u001b[33m: ffnn_rmse,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    384\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        \u001b[39m\u001b[33m'\u001b[39m\u001b[33msarima_rmse\u001b[39m\u001b[33m'\u001b[39m\u001b[33m: sarima_rmse if \u001b[39m\u001b[33m'\u001b[39m\u001b[33msarima_rmse\u001b[39m\u001b[33m'\u001b[39m\u001b[33m in locals() else None,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    385\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        \u001b[39m\u001b[33m'\u001b[39m\u001b[33mpermutation_importances\u001b[39m\u001b[33m'\u001b[39m\u001b[33m: importances\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    386\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    }\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    387\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    return results\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    388\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    389\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m# -----------------------------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    390\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m# ENTRY POINT\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    391\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m# -----------------------------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    392\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mif __name__ == \u001b[39m\u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    393\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    csv_path = \u001b[39m\u001b[33m'\u001b[39m\u001b[33m./lstm_timeseries_dataset.csv\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    394\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    generate_dataset(save_path=csv_path)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    395\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    # If GPU available, use it\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    396\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    device = \u001b[39m\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m\u001b[33m if torch.cuda.is_available() else \u001b[39m\u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    397\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    print(\u001b[39m\u001b[33m'\u001b[39m\u001b[33mUsing device:\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, device)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    398\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    results = run_full_pipeline(csv_path=csv_path, seq_len=30, test_size=0.2, device=device)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    399\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    print(\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mnSummary of key results:\u001b[39m\u001b[33m'\u001b[39m\u001b[33m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    400\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    print(results)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    401\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    print(\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mnNotes:\u001b[39m\u001b[33m'\u001b[39m\u001b[33m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    402\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    print(\u001b[39m\u001b[33m'\u001b[39m\u001b[33m - To reproduce exactly, ensure package versions are stable (see top of file).\u001b[39m\u001b[33m'\u001b[39m\u001b[33m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    403\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    print(\u001b[39m\u001b[33m'\u001b[39m\u001b[33m - Training time will depend on CPU/GPU; on a typical laptop CPU expect several minutes; on a GPU it may be under 10 minutes.\u001b[39m\u001b[33m'\u001b[39m\u001b[33m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    404\u001b[39m    ]\n\u001b[32m    405\u001b[39m   }\n\u001b[32m    406\u001b[39m  ],\n\u001b[32m    407\u001b[39m  \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m    408\u001b[39m   \u001b[33m\"\u001b[39m\u001b[33mkernelspec\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m    409\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mdisplay_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mPython [conda env:base] *\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    410\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mlanguage\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mpython\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    411\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mconda-base-py\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    412\u001b[39m   },\n\u001b[32m    413\u001b[39m   \u001b[33m\"\u001b[39m\u001b[33mlanguage_info\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m    414\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcodemirror_mode\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m    415\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mipython\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    416\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mversion\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m3\u001b[39m\n\u001b[32m    417\u001b[39m    },\n\u001b[32m    418\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mfile_extension\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m.py\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    419\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmimetype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mtext/x-python\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    420\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mpython\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    421\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mnbconvert_exporter\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mpython\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    422\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mpygments_lexer\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mipython3\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    423\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mversion\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m3.13.5\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    424\u001b[39m   }\n\u001b[32m    425\u001b[39m  },\n\u001b[32m    426\u001b[39m  \u001b[33m\"\u001b[39m\u001b[33mnbformat\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m4\u001b[39m,\n\u001b[32m    427\u001b[39m  \u001b[33m\"\u001b[39m\u001b[33mnbformat_minor\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m5\u001b[39m\n\u001b[32m    428\u001b[39m }\n",
      "\u001b[31mNameError\u001b[39m: name 'null' is not defined"
     ]
    }
   ],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"941de11d-01f5-4057-be26-82441549e2d1\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"\\\"\\\"\\\"\\n\",\n",
    "    \"Advanced Time Series Forecasting with Deep Learning: LSTM Optimization and Interpretability\\n\",\n",
    "    \"\\n\",\n",
    "    \"This single-file implementation covers:\\n\",\n",
    "    \"- Programmatic dataset generation (2000 daily observations)\\n\",\n",
    "    \"- Preprocessing and sequence dataset creation\\n\",\n",
    "    \"- PyTorch LSTM model with AdamW optimizer and cosine annealing LR schedule\\n\",\n",
    "    \"- Dropout scheduling (increasing dropout during training) and weight decay\\n\",\n",
    "    \"- Baseline models: SARIMA (statsmodels) and a simple Feedforward NN (PyTorch)\\n\",\n",
    "    \"- Evaluation: RMSE, MAE, MAPE on a hold-out test set\\n\",\n",
    "    \"- Interpretability: permutation importance over temporal features and a simple SHAP-like approximation\\n\",\n",
    "    \"- Saves dataset to ./lstm_timeseries_dataset.csv\\n\",\n",
    "    \"\\n\",\n",
    "    \"Run with: python Advanced_LSTM_Forecasting_Project.py\\n\",\n",
    "    \"\\n\",\n",
    "    \"Dependencies:\\n\",\n",
    "    \"- numpy, pandas, scipy, scikit-learn, torch, statsmodels, matplotlib\\n\",\n",
    "    \"- Optional: shap (only for advanced users)\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\\"\\\"\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"import math\\n\",\n",
    "    \"import random\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"from pathlib import Path\\n\",\n",
    "    \"from sklearn.preprocessing import StandardScaler\\n\",\n",
    "    \"from sklearn.metrics import mean_squared_error, mean_absolute_error\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"\\n\",\n",
    "    \"# PyTorch imports\\n\",\n",
    "    \"import torch\\n\",\n",
    "    \"import torch.nn as nn\\n\",\n",
    "    \"from torch.utils.data import Dataset, DataLoader\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Statsmodels for SARIMA baseline\\n\",\n",
    "    \"import statsmodels.api as sm\\n\",\n",
    "    \"\\n\",\n",
    "    \"# -----------------------------\\n\",\n",
    "    \"# 1) DATA GENERATION + SAVE\\n\",\n",
    "    \"# -----------------------------\\n\",\n",
    "    \"\\n\",\n",
    "    \"def generate_dataset(n=2000, seed=42, save_path=\\\"./lstm_timeseries_dataset.csv\\\"):\\n\",\n",
    "    \"    np.random.seed(seed)\\n\",\n",
    "    \"    date_index = pd.date_range(start=\\\"2000-01-01\\\", periods=n, freq=\\\"D\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Components\\n\",\n",
    "    \"    trend = 0.0005 * np.arange(n)               # slow linear trend\\n\",\n",
    "    \"    seasonal_yearly = 2.0 * np.sin(2 * np.pi * np.arange(n) / 365.25)\\n\",\n",
    "    \"    seasonal_weekly = 0.5 * np.sin(2 * np.pi * np.arange(n) / 7.0)\\n\",\n",
    "    \"    ar_component = np.zeros(n)\\n\",\n",
    "    \"    phi = [0.6, -0.2]\\n\",\n",
    "    \"    for t in range(2, n):\\n\",\n",
    "    \"        ar_component[t] = phi[0] * ar_component[t-1] + phi[1] * ar_component[t-2]\\n\",\n",
    "    \"\\n\",\n",
    "    \"    exog1 = 0.8 * np.sin(2 * np.pi * np.arange(n) / 90.0) + 0.2 * np.random.normal(size=n)\\n\",\n",
    "    \"    exog2 = np.cos(2 * np.pi * np.arange(n) / 30.0) + 0.1 * np.random.normal(size=n)\\n\",\n",
    "    \"    exog3 = np.random.normal(scale=0.5, size=n)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    noise = np.random.normal(scale=0.8, size=n)\\n\",\n",
    "    \"    target = 3.0 + trend + seasonal_yearly + seasonal_weekly + 1.5 * ar_component + 0.7 * exog1 - 0.4 * exog2 + 0.3 * exog3 + noise\\n\",\n",
    "    \"\\n\",\n",
    "    \"    max_lag = 14\\n\",\n",
    "    \"    lags = {f\\\"lag_{lag}\\\": pd.Series(target).shift(lag).fillna(method=\\\"bfill\\\").values for lag in range(1, max_lag+1)}\\n\",\n",
    "    \"\\n\",\n",
    "    \"    df = pd.DataFrame({\\n\",\n",
    "    \"        \\\"timestamp\\\": date_index,\\n\",\n",
    "    \"        \\\"target\\\": target,\\n\",\n",
    "    \"        \\\"exog1\\\": exog1,\\n\",\n",
    "    \"        \\\"exog2\\\": exog2,\\n\",\n",
    "    \"        \\\"exog3\\\": exog3,\\n\",\n",
    "    \"        \\\"trend\\\": trend,\\n\",\n",
    "    \"        \\\"seasonal_yearly\\\": seasonal_yearly,\\n\",\n",
    "    \"        \\\"seasonal_weekly\\\": seasonal_weekly\\n\",\n",
    "    \"    })\\n\",\n",
    "    \"    for k, v in lags.items():\\n\",\n",
    "    \"        df[k] = v\\n\",\n",
    "    \"\\n\",\n",
    "    \"    df.to_csv(save_path, index=False)\\n\",\n",
    "    \"    print(f\\\"Dataset saved to: {save_path}\\\")\\n\",\n",
    "    \"    return df\\n\",\n",
    "    \"\\n\",\n",
    "    \"# -----------------------------\\n\",\n",
    "    \"# 2) PYTORCH SEQUENCE DATASET\\n\",\n",
    "    \"# -----------------------------\\n\",\n",
    "    \"\\n\",\n",
    "    \"class SequenceDataset(Dataset):\\n\",\n",
    "    \"    def __init__(self, df, feature_cols, target_col='target', seq_len=30):\\n\",\n",
    "    \"        self.feature_cols = feature_cols\\n\",\n",
    "    \"        self.target_col = target_col\\n\",\n",
    "    \"        self.seq_len = seq_len\\n\",\n",
    "    \"        self.X = df[feature_cols].values.astype(np.float32)\\n\",\n",
    "    \"        self.y = df[target_col].values.astype(np.float32)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def __len__(self):\\n\",\n",
    "    \"        return len(self.y) - self.seq_len\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def __getitem__(self, idx):\\n\",\n",
    "    \"        x = self.X[idx: idx + self.seq_len]\\n\",\n",
    "    \"        y = self.y[idx + self.seq_len]\\n\",\n",
    "    \"        return x, y\\n\",\n",
    "    \"\\n\",\n",
    "    \"# -----------------------------\\n\",\n",
    "    \"# 3) LSTM MODEL (PyTorch)\\n\",\n",
    "    \"# -----------------------------\\n\",\n",
    "    \"\\n\",\n",
    "    \"class LSTMForecast(nn.Module):\\n\",\n",
    "    \"    def __init__(self, n_features, hidden_size=64, num_layers=2, dropout=0.2, bidirectional=False):\\n\",\n",
    "    \"        super().__init__()\\n\",\n",
    "    \"        self.lstm = nn.LSTM(input_size=n_features, hidden_size=hidden_size,\\n\",\n",
    "    \"                            num_layers=num_layers, batch_first=True, dropout=dropout, bidirectional=bidirectional)\\n\",\n",
    "    \"        direction = 2 if bidirectional else 1\\n\",\n",
    "    \"        self.fc = nn.Sequential(\\n\",\n",
    "    \"            nn.Linear(hidden_size * direction, 64),\\n\",\n",
    "    \"            nn.ReLU(),\\n\",\n",
    "    \"            nn.Dropout(dropout),\\n\",\n",
    "    \"            nn.Linear(64, 1)\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def forward(self, x):\\n\",\n",
    "    \"        # x: (batch, seq_len, features)\\n\",\n",
    "    \"        out, _ = self.lstm(x)\\n\",\n",
    "    \"        out = out[:, -1, :]  # last time step\\n\",\n",
    "    \"        out = self.fc(out)\\n\",\n",
    "    \"        return out.squeeze(1)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# -----------------------------\\n\",\n",
    "    \"# Utility: train / evaluate\\n\",\n",
    "    \"# -----------------------------\\n\",\n",
    "    \"\\n\",\n",
    "    \"def train_model(model, train_loader, val_loader, epochs=50, lr=1e-3, weight_decay=1e-4, device='cpu', dropout_schedule=None):\\n\",\n",
    "    \"    model.to(device)\\n\",\n",
    "    \"    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\\n\",\n",
    "    \"    # Cosine annealing scheduler\\n\",\n",
    "    \"    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\\n\",\n",
    "    \"    loss_fn = nn.MSELoss()\\n\",\n",
    "    \"\\n\",\n",
    "    \"    best_val = float('inf')\\n\",\n",
    "    \"    best_state = None\\n\",\n",
    "    \"\\n\",\n",
    "    \"    for ep in range(1, epochs+1):\\n\",\n",
    "    \"        model.train()\\n\",\n",
    "    \"        train_losses = []\\n\",\n",
    "    \"        # optional dropout schedule: a function mapping epoch->dropout value\\n\",\n",
    "    \"        if dropout_schedule is not None and hasattr(model, 'lstm'):\\n\",\n",
    "    \"            new_dropout = float(dropout_schedule(ep))\\n\",\n",
    "    \"            # update dropout in LSTM and fc dropout layers (if present)\\n\",\n",
    "    \"            try:\\n\",\n",
    "    \"                model.lstm.dropout = new_dropout\\n\",\n",
    "    \"            except Exception:\\n\",\n",
    "    \"                pass\\n\",\n",
    "    \"            # update fc dropout layers\\n\",\n",
    "    \"            for m in model.modules():\\n\",\n",
    "    \"                if isinstance(m, nn.Dropout):\\n\",\n",
    "    \"                    m.p = new_dropout\\n\",\n",
    "    \"\\n\",\n",
    "    \"        for xb, yb in train_loader:\\n\",\n",
    "    \"            xb = xb.to(device)\\n\",\n",
    "    \"            yb = yb.to(device)\\n\",\n",
    "    \"            optimizer.zero_grad()\\n\",\n",
    "    \"            preds = model(xb)\\n\",\n",
    "    \"            loss = loss_fn(preds, yb)\\n\",\n",
    "    \"            loss.backward()\\n\",\n",
    "    \"            optimizer.step()\\n\",\n",
    "    \"            train_losses.append(loss.item())\\n\",\n",
    "    \"\\n\",\n",
    "    \"        scheduler.step()\\n\",\n",
    "    \"\\n\",\n",
    "    \"        # validation\\n\",\n",
    "    \"        model.eval()\\n\",\n",
    "    \"        val_losses = []\\n\",\n",
    "    \"        with torch.no_grad():\\n\",\n",
    "    \"            for xb, yb in val_loader:\\n\",\n",
    "    \"                xb = xb.to(device)\\n\",\n",
    "    \"                yb = yb.to(device)\\n\",\n",
    "    \"                preds = model(xb)\\n\",\n",
    "    \"                val_losses.append(mean_squared_error(yb.cpu().numpy(), preds.cpu().numpy()))\\n\",\n",
    "    \"        val_rmse = math.sqrt(np.mean(val_losses))\\n\",\n",
    "    \"\\n\",\n",
    "    \"        if val_rmse < best_val:\\n\",\n",
    "    \"            best_val = val_rmse\\n\",\n",
    "    \"            best_state = model.state_dict()\\n\",\n",
    "    \"\\n\",\n",
    "    \"        if ep % 10 == 0 or ep == 1:\\n\",\n",
    "    \"            print(f\\\"Epoch {ep}/{epochs} - train_loss: {np.mean(train_losses):.4f} - val_rmse: {val_rmse:.4f}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"    if best_state is not None:\\n\",\n",
    "    \"        model.load_state_dict(best_state)\\n\",\n",
    "    \"    return model\\n\",\n",
    "    \"\\n\",\n",
    "    \"# -----------------------------\\n\",\n",
    "    \"# Baselines: SARIMA and FFNN\\n\",\n",
    "    \"# -----------------------------\\n\",\n",
    "    \"\\n\",\n",
    "    \"def sarima_forecast(train_series, test_steps, order=(2,0,1), seasonal_order=(1,1,1,365)):\\n\",\n",
    "    \"    # Note: seasonal_order's seasonal_periods is large (365) because daily data has yearly seasonality\\n\",\n",
    "    \"    model = sm.tsa.statespace.SARIMAX(train_series, order=order, seasonal_order=seasonal_order, enforce_stationarity=False, enforce_invertibility=False)\\n\",\n",
    "    \"    res = model.fit(disp=False)\\n\",\n",
    "    \"    fc = res.get_forecast(steps=test_steps).predicted_mean\\n\",\n",
    "    \"    return fc\\n\",\n",
    "    \"\\n\",\n",
    "    \"class FFNN(nn.Module):\\n\",\n",
    "    \"    def __init__(self, input_dim, hidden=128, dropout=0.2):\\n\",\n",
    "    \"        super().__init__()\\n\",\n",
    "    \"        self.net = nn.Sequential(\\n\",\n",
    "    \"            nn.Linear(input_dim, hidden),\\n\",\n",
    "    \"            nn.ReLU(),\\n\",\n",
    "    \"            nn.Dropout(dropout),\\n\",\n",
    "    \"            nn.Linear(hidden, hidden//2),\\n\",\n",
    "    \"            nn.ReLU(),\\n\",\n",
    "    \"            nn.Linear(hidden//2, 1)\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"    def forward(self, x):\\n\",\n",
    "    \"        return self.net(x).squeeze(1)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# -----------------------------\\n\",\n",
    "    \"# Permutation Importance for sequences\\n\",\n",
    "    \"# -----------------------------\\n\",\n",
    "    \"\\n\",\n",
    "    \"def permutation_importance_sequence(model, X_test, y_test, feature_idx_to_permute, seq_len=30, device='cpu'):\\n\",\n",
    "    \"    # X_test: numpy array shape (n_samples, seq_len, n_features)\\n\",\n",
    "    \"    baseline_preds = model(torch.tensor(X_test, dtype=torch.float32).to(device)).detach().cpu().numpy()\\n\",\n",
    "    \"    baseline_rmse = math.sqrt(mean_squared_error(y_test, baseline_preds))\\n\",\n",
    "    \"\\n\",\n",
    "    \"    X_perm = X_test.copy()\\n\",\n",
    "    \"    # permute along the sample axis while preserving time order inside each sample for that feature index\\n\",\n",
    "    \"    for i in range(X_perm.shape[0]):\\n\",\n",
    "    \"        # shuffle the feature values across samples\\n\",\n",
    "    \"        X_perm[:, :, feature_idx_to_permute] = np.random.permutation(X_perm[:, :, feature_idx_to_permute])\\n\",\n",
    "    \"\\n\",\n",
    "    \"    perm_preds = model(torch.tensor(X_perm, dtype=torch.float32).to(device)).detach().cpu().numpy()\\n\",\n",
    "    \"    perm_rmse = math.sqrt(mean_squared_error(y_test, perm_preds))\\n\",\n",
    "    \"    importance = perm_rmse - baseline_rmse\\n\",\n",
    "    \"    return importance, baseline_rmse, perm_rmse\\n\",\n",
    "    \"\\n\",\n",
    "    \"# -----------------------------\\n\",\n",
    "    \"# FULL PIPELINE: load/generate data, preprocess, train, evaluate\\n\",\n",
    "    \"# -----------------------------\\n\",\n",
    "    \"\\n\",\n",
    "    \"def run_full_pipeline(csv_path='./lstm_timeseries_dataset.csv', seq_len=30, test_size=0.2, device='cpu'):\\n\",\n",
    "    \"    # ensure dataset exists\\n\",\n",
    "    \"    if not os.path.exists(csv_path):\\n\",\n",
    "    \"        print(\\\"Dataset not found locally â€” generating now...\\\")\\n\",\n",
    "    \"        df = generate_dataset(save_path=csv_path)\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        df = pd.read_csv(csv_path, parse_dates=['timestamp'])\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # features to use for sequence model (exclude timestamp and target)\\n\",\n",
    "    \"    feature_cols = [c for c in df.columns if c not in ['timestamp', 'target']]\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Split train/val/test by time (no shuffling)\\n\",\n",
    "    \"    n = len(df)\\n\",\n",
    "    \"    test_n = int(n * test_size)\\n\",\n",
    "    \"    train_val_n = n - test_n\\n\",\n",
    "    \"    val_n = int(train_val_n * 0.1)\\n\",\n",
    "    \"    train_n = train_val_n - val_n\\n\",\n",
    "    \"\\n\",\n",
    "    \"    train_df = df.iloc[:train_n].copy()\\n\",\n",
    "    \"    val_df = df.iloc[train_n:train_n+val_n].copy()\\n\",\n",
    "    \"    test_df = df.iloc[train_n+val_n:].copy()\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # scaler fitted on train features only\\n\",\n",
    "    \"    scaler = StandardScaler()\\n\",\n",
    "    \"    scaler.fit(train_df[feature_cols].values)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    for d in [train_df, val_df, test_df]:\\n\",\n",
    "    \"        d[feature_cols] = scaler.transform(d[feature_cols].values)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    train_ds = SequenceDataset(train_df, feature_cols, seq_len=seq_len)\\n\",\n",
    "    \"    val_ds = SequenceDataset(val_df, feature_cols, seq_len=seq_len)\\n\",\n",
    "    \"    test_ds = SequenceDataset(test_df, feature_cols, seq_len=seq_len)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    batch_size = 64\\n\",\n",
    "    \"    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\\n\",\n",
    "    \"    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\\n\",\n",
    "    \"    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    n_features = len(feature_cols)\\n\",\n",
    "    \"    model = LSTMForecast(n_features=n_features, hidden_size=128, num_layers=2, dropout=0.2, bidirectional=False)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # dropout schedule: increase dropout slowly to regularize later epochs\\n\",\n",
    "    \"    def dropout_schedule(epoch):\\n\",\n",
    "    \"        return min(0.5, 0.1 + (epoch / 100) * 0.4)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    model = train_model(model, train_loader, val_loader, epochs=60, lr=1e-3, weight_decay=1e-4, device=device, dropout_schedule=dropout_schedule)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Evaluate on test set\\n\",\n",
    "    \"    model.to(device)\\n\",\n",
    "    \"    model.eval()\\n\",\n",
    "    \"    X_test = []\\n\",\n",
    "    \"    y_test = []\\n\",\n",
    "    \"    with torch.no_grad():\\n\",\n",
    "    \"        for xb, yb in test_loader:\\n\",\n",
    "    \"            X_test.append(xb.numpy())\\n\",\n",
    "    \"            y_test.append(yb.numpy())\\n\",\n",
    "    \"    X_test = np.concatenate(X_test, axis=0)\\n\",\n",
    "    \"    y_test = np.concatenate(y_test, axis=0)\\n\",\n",
    "    \"    preds = model(torch.tensor(X_test, dtype=torch.float32).to(device)).detach().cpu().numpy()\\n\",\n",
    "    \"\\n\",\n",
    "    \"    rmse = math.sqrt(mean_squared_error(y_test, preds))\\n\",\n",
    "    \"    mae = mean_absolute_error(y_test, preds)\\n\",\n",
    "    \"    mape = np.mean(np.abs((y_test - preds) / (y_test + 1e-8))) * 100\\n\",\n",
    "    \"\\n\",\n",
    "    \"    print(f\\\"LSTM Test RMSE: {rmse:.4f}, MAE: {mae:.4f}, MAPE: {mape:.2f}%\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Baseline 1: SARIMA trained on the unscaled target series (last portion as test)\\n\",\n",
    "    \"    train_series = df['target'].iloc[:train_n+val_n]\\n\",\n",
    "    \"    test_steps = len(test_df) - seq_len\\n\",\n",
    "    \"    try:\\n\",\n",
    "    \"        sarima_pred = sarima_forecast(train_series.values, test_steps=test_steps)\\n\",\n",
    "    \"        # align length\\n\",\n",
    "    \"        sarima_y = df['target'].iloc[train_n+val_n+seq_len:train_n+val_n+seq_len+test_steps].values\\n\",\n",
    "    \"        sarima_rmse = math.sqrt(mean_squared_error(sarima_y, sarima_pred))\\n\",\n",
    "    \"        print(f\\\"SARIMA Test RMSE (approx): {sarima_rmse:.4f}\\\")\\n\",\n",
    "    \"    except Exception as e:\\n\",\n",
    "    \"        print(\\\"SARIMA failed (likely because statsmodels couldn't converge).\\\", e)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Baseline 2: Feedforward NN on lag features (flatten last seq)\\n\",\n",
    "    \"    # Prepare flattened lag features: use last seq_len timesteps flattened\\n\",\n",
    "    \"    X_flat = []\\n\",\n",
    "    \"    y_flat = []\\n\",\n",
    "    \"    for i in range(len(df) - seq_len):\\n\",\n",
    "    \"        X_flat.append(df[feature_cols].values[i:i+seq_len].flatten())\\n\",\n",
    "    \"        y_flat.append(df['target'].values[i+seq_len])\\n\",\n",
    "    \"    X_flat = np.array(X_flat)\\n\",\n",
    "    \"    y_flat = np.array(y_flat)\\n\",\n",
    "    \"    # train/val/test splits aligned\\n\",\n",
    "    \"    X_train, X_val, X_test_flat = X_flat[:train_n-seq_len], X_flat[train_n-seq_len:train_n+val_n-seq_len], X_flat[train_n+val_n-seq_len:]\\n\",\n",
    "    \"    y_train, y_val, y_test_flat = y_flat[:train_n-seq_len], y_flat[train_n-seq_len:train_n+val_n-seq_len], y_flat[train_n+val_n-seq_len:]\\n\",\n",
    "    \"\\n\",\n",
    "    \"    ffnn = FFNN(input_dim=X_train.shape[1], hidden=256, dropout=0.3)\\n\",\n",
    "    \"    optimizer = torch.optim.AdamW(ffnn.parameters(), lr=1e-3, weight_decay=1e-4)\\n\",\n",
    "    \"    loss_fn = nn.MSELoss()\\n\",\n",
    "    \"    ffnn.to(device)\\n\",\n",
    "    \"    # small training loop\\n\",\n",
    "    \"    for epoch in range(25):\\n\",\n",
    "    \"        ffnn.train()\\n\",\n",
    "    \"        perm = np.random.permutation(len(X_train))\\n\",\n",
    "    \"        batch = 128\\n\",\n",
    "    \"        losses = []\\n\",\n",
    "    \"        for i in range(0, len(perm), batch):\\n\",\n",
    "    \"            idx = perm[i:i+batch]\\n\",\n",
    "    \"            xb = torch.tensor(X_train[idx], dtype=torch.float32).to(device)\\n\",\n",
    "    \"            yb = torch.tensor(y_train[idx], dtype=torch.float32).to(device)\\n\",\n",
    "    \"            optimizer.zero_grad()\\n\",\n",
    "    \"            preds = ffnn(xb)\\n\",\n",
    "    \"            loss = loss_fn(preds, yb)\\n\",\n",
    "    \"            loss.backward()\\n\",\n",
    "    \"            optimizer.step()\\n\",\n",
    "    \"            losses.append(loss.item())\\n\",\n",
    "    \"        if epoch % 5 == 0:\\n\",\n",
    "    \"            print(f\\\"FFNN epoch {epoch} loss: {np.mean(losses):.4f}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"    ffnn.eval()\\n\",\n",
    "    \"    with torch.no_grad():\\n\",\n",
    "    \"        pred_ffnn = ffnn(torch.tensor(X_test_flat, dtype=torch.float32).to(device)).cpu().numpy()\\n\",\n",
    "    \"    ffnn_rmse = math.sqrt(mean_squared_error(y_test_flat, pred_ffnn))\\n\",\n",
    "    \"    print(f\\\"FFNN (lag-flattened) RMSE: {ffnn_rmse:.4f}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Permutation importance on LSTM (feature-level)\\n\",\n",
    "    \"    print(\\\"Computing permutation importances (this may take a little while)...\\\")\\n\",\n",
    "    \"    importances = {}\\n\",\n",
    "    \"    for i, feat in enumerate(feature_cols):\\n\",\n",
    "    \"        imp, base, perm = permutation_importance_sequence(model, X_test, y_test, feature_idx_to_permute=i, seq_len=seq_len, device=device)\\n\",\n",
    "    \"        importances[feat] = imp\\n\",\n",
    "    \"    sorted_imps = sorted(importances.items(), key=lambda x: -abs(x[1]))\\n\",\n",
    "    \"    print(\\\"Permutation importances (feature -> delta RMSE):\\\")\\n\",\n",
    "    \"    for feat, imp in sorted_imps:\\n\",\n",
    "    \"        print(f\\\"  {feat}: {imp:.4f}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"    results = {\\n\",\n",
    "    \"        'lstm_rmse': rmse,\\n\",\n",
    "    \"        'ffnn_rmse': ffnn_rmse,\\n\",\n",
    "    \"        'sarima_rmse': sarima_rmse if 'sarima_rmse' in locals() else None,\\n\",\n",
    "    \"        'permutation_importances': importances\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"    return results\\n\",\n",
    "    \"\\n\",\n",
    "    \"# -----------------------------\\n\",\n",
    "    \"# ENTRY POINT\\n\",\n",
    "    \"# -----------------------------\\n\",\n",
    "    \"if __name__ == '__main__':\\n\",\n",
    "    \"    csv_path = './lstm_timeseries_dataset.csv'\\n\",\n",
    "    \"    generate_dataset(save_path=csv_path)\\n\",\n",
    "    \"    # If GPU available, use it\\n\",\n",
    "    \"    device = 'cuda' if torch.cuda.is_available() else 'cpu'\\n\",\n",
    "    \"    print('Using device:', device)\\n\",\n",
    "    \"    results = run_full_pipeline(csv_path=csv_path, seq_len=30, test_size=0.2, device=device)\\n\",\n",
    "    \"    print('\\\\nSummary of key results:')\\n\",\n",
    "    \"    print(results)\\n\",\n",
    "    \"    print('\\\\nNotes:')\\n\",\n",
    "    \"    print(' - To reproduce exactly, ensure package versions are stable (see top of file).')\\n\",\n",
    "    \"    print(' - Training time will depend on CPU/GPU; on a typical laptop CPU expect several minutes; on a GPU it may be under 10 minutes.')\\n\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python [conda env:base] *\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"conda-base-py\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.13.5\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 5\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
